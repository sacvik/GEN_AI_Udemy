{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook looks at different preprocessing techniques and basic data clensing. \n",
    "Based on the cleaned data we have tried various methods to extract features.\n",
    "We also have tried to test multiple ML/ANN models to test the accuracy on this data.\n",
    "\n",
    "Next:\n",
    "    We will look into this data again and try to clean it more so that we can achieve\n",
    "     better accuracy, recall and precision.\n",
    "\n",
    "     It is left for user to look into the data closely and crate better corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data\n",
    "\n",
    "utube_df_a = pd.read_csv('../Data/YoutubeCommentsDataSet.csv')\n",
    "utube_df = utube_df_a[~utube_df_a.Comment.isnull()].reset_index(drop=True)\n",
    "# utube_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for class imbalance\n",
    "utube_df.Sentiment.value_counts(), utube_df_a.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer_snowball = PorterStemmer()\n",
    "\n",
    "utube_corpus_snow = []\n",
    "rejected_snow = []\n",
    "for i in range(len(utube_df)):\n",
    "    try:\n",
    "        sentance = re.sub(\"[^a-zA-z]\", \" \", utube_df['Comment'][i]).lower().split()\n",
    "        sentance = [stemmer_snowball.stem(word) for word in sentance if word not in stopwords.words('english')]\n",
    "        sentance = ' '.join(sentance)\n",
    "        utube_corpus_snow.append(sentance)\n",
    "    except:\n",
    "        print(i)\n",
    "        rejected_snow.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use lemmatizer for standardizing the words across\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "utube_corpus = []\n",
    "rejected = []\n",
    "for i in range(len(utube_df)):\n",
    "    try:\n",
    "        sentance = re.sub(\"[^a-zA-z]\", \" \", utube_df['Comment'][i]).lower().split()\n",
    "        sentance = [lemmatizer.lemmatize(word) for word in sentance if word not in stopwords.words('english')]\n",
    "        sentance = ' '.join(sentance)\n",
    "        utube_corpus.append(sentance)\n",
    "    except:\n",
    "        print(i)\n",
    "        rejected.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature creation Using BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=10000, ngram_range=(1,4))\n",
    "X = cv.fit_transform(utube_corpus).toarray()\n",
    "Y = utube_df['Sentiment'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utube_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train, test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=0)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.combine import SMOTEENN\n",
    "# from collections import Counter\n",
    "\n",
    "# # Applying SMOTE + ENN\n",
    "# smote_enn = SMOTEENN(random_state=42)\n",
    "# X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "# print(\"Class Distribution After SMOTEENN:\", Counter(y_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Applying SMOTE + ENN\n",
    "smote = SMOTE(random_state=42, sampling_strategy='not majority')\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Class Distribution After SMOTEENN:\", Counter(y_resampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - model multinominal logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model_logistic = LogisticRegression(solver='lbfgs', \n",
    "                                    multi_class='multinomial', max_iter=500)\\\n",
    "                                        .fit(X_train, y_train)\n",
    "y_train_pred_prob = model_logistic.predict_proba(X_train)\n",
    "y_test_pred_prob = model_logistic.predict_proba(X_test)\n",
    "\n",
    "y_train_pred = model_logistic.predict(X_train)\n",
    "y_test_pred = model_logistic.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - model Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_randomforest = RandomForestClassifier(class_weight='balanced', n_estimators=100, criterion='log_loss',\n",
    "                                            random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_prob = model_randomforest.predict_proba(X_train)\n",
    "y_test_pred_prob = model_randomforest.predict_proba(X_test)\n",
    "\n",
    "y_train_pred = model_randomforest.predict(X_train)\n",
    "y_test_pred = model_randomforest.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - model multinominal XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.bincount(Y)  # Get number of samples per class\n",
    "total_samples = len(Y)  # Total number of samples\n",
    "\n",
    "# Compute class-wise weights\n",
    "class_weights = {i: (total_samples - class_counts[i]) / class_counts[i] for i in range(len(class_counts))}\n",
    "max_weight = max(class_weights.values())\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# Assign sample weights based on class\n",
    "sample_weights = np.array([class_weights[label] for label in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# XGBoost with class weight adjustment\n",
    "xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(class_counts), scale_pos_weight=max_weight)\n",
    "xgb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_test_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_test_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - using back propogation with ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### update data for keras\n",
    "y_train_k = keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_train_k_resampled = keras.utils.to_categorical(y_resampled, num_classes=3)\n",
    "y_test_k = keras.utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann = Sequential()\n",
    "model_ann.add(Dense(1000, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model_ann.add(Dropout(0.1))\n",
    "model_ann.add(Dense(200, activation='relu'))\n",
    "model_ann.add(Dropout(0.1))\n",
    "model_ann.add(Dense(3, activation='softmax'))\n",
    "model_ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann.compile(optimizer=RMSprop(),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_ann.fit(X_train, y_train_k, \n",
    "                    epochs=10, batch_size=32, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r*-', label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r*-', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model_ann.predict(X_train)\n",
    "y_train_pred = np.argmax(y_train_pred, axis=1) \n",
    "y_test_pred = model_ann.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "y_test_labels = np.argmax(y_test_k, axis=1)\n",
    "y_train_labels = np.argmax(y_train_k, axis=1)\n",
    "\n",
    "print(accuracy_score(y_test, y_test_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(classification_report(y_test_labels, y_test_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(classification_report(y_train_labels, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature creation using Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### first, lets train word2vec model using our data.\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "## break sentances in list of words.\n",
    "sentances_to_word = [x.split() for x in utube_corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentances_to_word, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save model (Optional)\n",
    "word2vec_model.save(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### feature creation\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Download and Load Pretrained Google News Word2Vec Model (300-Dimensional)\n",
    "word2vec_model_google = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Check Word Vector for \"learning\"\n",
    "# print(word2vec_model['learning'][:10])  # Print first 10 dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence, model):\n",
    "    words = sentence.split()  # Tokenize sentence\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([get_sentence_vector(sent, word2vec_model_google) for sent in utube_corpus])\n",
    "Y = utube_df['Sentiment'].astype('category').cat.codes\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train, test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=0)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Applying SMOTE + ENN\n",
    "smote = SMOTE(random_state=42, sampling_strategy='not majority')\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Class Distribution After SMOTEENN:\", Counter(y_resampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - model using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model_logistic = LogisticRegression(solver='lbfgs', \n",
    "                                    multi_class='multinomial', max_iter=500)\\\n",
    "                                        .fit(X_train, y_train)\n",
    "y_train_pred_prob = model_logistic.predict_proba(X_train)\n",
    "y_test_pred_prob = model_logistic.predict_proba(X_test)\n",
    "\n",
    "y_train_pred = model_logistic.predict(X_train)\n",
    "y_test_pred = model_logistic.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - model using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### update data for keras\n",
    "y_train_k = keras.utils.to_categorical(y_train, num_classes=3)\n",
    "y_train_k_resampled = keras.utils.to_categorical(y_resampled, num_classes=3)\n",
    "y_test_k = keras.utils.to_categorical(y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ann = Sequential()\n",
    "model_ann.add(Dense(300, activation='relu', input_shape=(300,)))\n",
    "# model_ann.add(Dropout(0.1))\n",
    "model_ann.add(Dense(50, activation='relu'))\n",
    "# model_ann.add(Dropout(0.1))\n",
    "model_ann.add(Dense(3, activation='softmax'))\n",
    "model_ann.summary()\n",
    "\n",
    "model_ann.compile(optimizer=RMSprop(),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model train\n",
    "history = model_ann.fit(X_train, y_train_k, \n",
    "                    epochs=10, batch_size=32, \n",
    "                    validation_data=(X_test, y_test_k),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r*-', label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r*-', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model_ann.predict(X_train)\n",
    "y_train_pred = np.argmax(y_train_pred, axis=1) \n",
    "y_test_pred = model_ann.predict(X_test)\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "\n",
    "y_test_labels = np.argmax(y_test_k, axis=1)\n",
    "y_train_labels = np.argmax(y_train_k, axis=1)\n",
    "\n",
    "print(accuracy_score(y_test, y_test_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(classification_report(y_test_labels, y_test_pred))\n",
    "print(\"\\n\\n\")\n",
    "print(classification_report(y_train_labels, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
